{
    "name": "model",
    "atomic": "model",
    "type": "Module",
    "input": "(1, 1)",
    "output": "(1, 1, 50304)",
    "submodules": [
        {
            "name": "gpt_neox",
            "atomic": ".gpt_neox",
            "type": "Module",
            "input": "(1, 1)",
            "output": "(1, 1, 128)",
            "submodules": [
                {
                    "name": "embed_in",
                    "atomic": ".gpt_neox.embed_in",
                    "type": "Module",
                    "input": "(1, 1)",
                    "output": "(1, 1, 128)"
                },
                {
                    "name": "layers",
                    "atomic": ".gpt_neox.layers",
                    "type": "ModuleList",
                    "input": null,
                    "output": null,
                    "submodules": [
                        {
                            "name": "layers.0",
                            "atomic": ".gpt_neox.layers.0",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.0.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.0.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.0.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.0.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.0.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.0.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.0.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.0.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.0.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.0.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "layers.1",
                            "atomic": ".gpt_neox.layers.1",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.1.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.1.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.1.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.1.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.1.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.1.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.1.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.1.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.1.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.1.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "layers.2",
                            "atomic": ".gpt_neox.layers.2",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.2.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.2.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.2.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.2.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.2.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.2.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.2.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.2.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.2.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.2.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "layers.3",
                            "atomic": ".gpt_neox.layers.3",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.3.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.3.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.3.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.3.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.3.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.3.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.3.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.3.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.3.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.3.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "layers.4",
                            "atomic": ".gpt_neox.layers.4",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.4.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.4.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.4.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.4.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.4.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.4.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.4.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.4.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.4.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.4.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "name": "layers.5",
                            "atomic": ".gpt_neox.layers.5",
                            "type": "Module",
                            "input": "(1, 1, 128)",
                            "output": "(1, 1, 128)",
                            "submodules": [
                                {
                                    "name": "input_layernorm",
                                    "atomic": ".gpt_neox.layers.5.input_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "post_attention_layernorm",
                                    "atomic": ".gpt_neox.layers.5.post_attention_layernorm",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)"
                                },
                                {
                                    "name": "attention",
                                    "atomic": ".gpt_neox.layers.5.attention",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "rotary_emb",
                                            "atomic": ".gpt_neox.layers.5.attention.rotary_emb",
                                            "type": "Module",
                                            "input": "(1, 4, 1, 32)",
                                            "output": "(1, 8)"
                                        },
                                        {
                                            "name": "query_key_value",
                                            "atomic": ".gpt_neox.layers.5.attention.query_key_value",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 384)"
                                        },
                                        {
                                            "name": "dense",
                                            "atomic": ".gpt_neox.layers.5.attention.dense",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 128)"
                                        }
                                    ]
                                },
                                {
                                    "name": "mlp",
                                    "atomic": ".gpt_neox.layers.5.mlp",
                                    "type": "Module",
                                    "input": "(1, 1, 128)",
                                    "output": "(1, 1, 128)",
                                    "submodules": [
                                        {
                                            "name": "dense_h_to_4h",
                                            "atomic": ".gpt_neox.layers.5.mlp.dense_h_to_4h",
                                            "type": "Module",
                                            "input": "(1, 1, 128)",
                                            "output": "(1, 1, 512)"
                                        },
                                        {
                                            "name": "dense_4h_to_h",
                                            "atomic": ".gpt_neox.layers.5.mlp.dense_4h_to_h",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 128)"
                                        },
                                        {
                                            "name": "act",
                                            "atomic": ".gpt_neox.layers.5.mlp.act",
                                            "type": "Module",
                                            "input": "(1, 1, 512)",
                                            "output": "(1, 1, 512)"
                                        }
                                    ]
                                }
                            ]
                        }
                    ]
                },
                {
                    "name": "final_layer_norm",
                    "atomic": ".gpt_neox.final_layer_norm",
                    "type": "Module",
                    "input": "(1, 1, 128)",
                    "output": "(1, 1, 128)"
                }
            ]
        },
        {
            "name": "embed_out",
            "atomic": ".embed_out",
            "type": "Module",
            "input": "(1, 1, 128)",
            "output": "(1, 1, 50304)"
        }
    ]
}